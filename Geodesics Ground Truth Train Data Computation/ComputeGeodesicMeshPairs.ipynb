{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ecf66db-a4fa-445d-8698-cf4a90e8c93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def split_meshes(num_meshes, val_ratio=0.025):\n",
    "    indices = list(range(1, num_meshes + 1))  # Mesh indices from 1 to num_meshes\n",
    "    random.shuffle(indices)\n",
    "    val_size = int(num_meshes * val_ratio)\n",
    "    val_set = indices[:val_size]\n",
    "    train_set = indices[val_size:]\n",
    "    return train_set, val_set\n",
    "\n",
    "def sample_mesh_pairs_from_set(mesh_set, num_pairs):\n",
    "    pairs = set()\n",
    "    while len(pairs) < num_pairs:\n",
    "        a = random.choice(mesh_set)\n",
    "        b = random.choice(mesh_set)\n",
    "        #if a == b:\n",
    "        #    continue\n",
    "        #if a > b:\n",
    "        #    a, b = b, a\n",
    "        pairs.add((a, b))\n",
    "    return list(pairs)\n",
    "\n",
    "\n",
    "\n",
    "def sample_sources_and_dests(num_vertices, num_sources, num_dests_per_source):\n",
    "    sources = random.sample(range(num_vertices), num_sources)\n",
    "\n",
    "    # Repeat each source num_dests_per_source times (we don't need source values, just their count)\n",
    "    total_dests = num_sources * num_dests_per_source\n",
    "\n",
    "    # Sample destinations for all sources at once\n",
    "    dests = np.random.randint(0, num_vertices, size=total_dests).tolist()\n",
    "\n",
    "    return sources, dests\n",
    "\n",
    "\n",
    "def write_list_to_file(lst, fn):\n",
    "    #with open(fn, \"w\") as f:\n",
    "    #    f.write(f\"{len(lst)}\\n\")  # Write the count first\n",
    "    #    f.write(\"\\n\".join(str(x) for x in lst))\n",
    "    lst = np.array(lst)\n",
    "    lst = np.insert(lst, 0, len(lst))\n",
    "    np.savetxt(fn, np.array(lst),  fmt='%d')\n",
    "\n",
    "def read_floats(fn):\n",
    "    return np.loadtxt(fn)[1:]\n",
    "    #with open(fn) as f:\n",
    "    #    return np.fromiter((float(line) for line in f), dtype=np.float32)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb55a0-fc4b-4f29-a908-7fef3f6c4803",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "NUM_MESHES = 12000\n",
    "NUM_PAIRS = 40000\n",
    "NUM_VERTICES = 3889\n",
    "NUM_SOURCES = 400\n",
    "NUM_DESTS_PER_SOURCE = 10\n",
    "# You can do random sample first using split_meshes and save the train & val indexes if you have not computed them. This code allows us to create multiple train dataset from the same train & val split.\n",
    "train_set, val_set = np.load(\"TrainIndexPairs.npy\"), np.load(\"ValIndexPairs.npy\")#split_meshes(NUM_MESHES, val_ratio=0.025) \n",
    "#np.save(\"TrainIndexPairs.npy\", train_set)\n",
    "#np.save(\"ValIndexPairs.npy\", val_set)\n",
    "train_pairs = sample_mesh_pairs_from_set(train_set, num_pairs=NUM_PAIRS)\n",
    "#PAIRS = sample_mesh_pairs(NUM_MESHES, NUM_PAIRS)\n",
    "\n",
    "# total rows = NUM_PAIRS * NUM_SOURCES * NUM_DESTS_PER_SOURCE\n",
    "total = NUM_PAIRS * NUM_SOURCES * NUM_DESTS_PER_SOURCE\n",
    "\n",
    "# For integer arrays, use -1 as a sentinel (assuming valid indices are non-negative)\n",
    "mesh_a = np.full(total, -1, dtype=np.int16)\n",
    "mesh_b = np.full(total, -1, dtype=np.int16)\n",
    "sources = np.full(total, -1, dtype=np.int16)\n",
    "dests   = np.full(total, -1, dtype=np.int16)\n",
    "\n",
    "# For float arrays, use np.nan as a sentinel\n",
    "dist_a  = np.full(total, np.nan, dtype=np.float16)\n",
    "dist_b  = np.full(total, np.nan, dtype=np.float16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c443bcd6-5a3c-4e34-bb7c-451dececec70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mesh‑pairs: 100%|██████████████████████████████████████████████████████████████| 40000/40000 [7:22:01<00:00,  1.51it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# If you run out of RAM, uncomment below and comment out the in‑memory lines above:\n",
    "# mesh_a = np.memmap(\"mesh_a.dat\", dtype=np.int16, mode=\"w+\", shape=(total,))\n",
    "# mesh_b = np.memmap(\"mesh_b.dat\", dtype=np.int16, mode=\"w+\", shape=(total,))\n",
    "# sources = np.memmap(\"sources.dat\", dtype=np.int16, mode=\"w+\", shape=(total,))\n",
    "# dests   = np.memmap(\"dests.dat\",   dtype=np.int16, mode=\"w+\", shape=(total,))\n",
    "# dist_a  = np.memmap(\"dist_a.dat\",  dtype=np.float32, mode=\"w+\", shape=(total,))\n",
    "# dist_b  = np.memmap(\"dist_b.dat\",  dtype=np.float32, mode=\"w+\", shape=(total,))\n",
    "\n",
    "idx = 0\n",
    "pid = 0\n",
    "for ma, mb in tqdm(train_pairs, desc=\"mesh‑pairs\"):\n",
    "    srcs, dsts = sample_sources_and_dests(NUM_VERTICES, NUM_SOURCES, NUM_DESTS_PER_SOURCE)\n",
    "    write_list_to_file(srcs, f\"source_{pid}.txt\")\n",
    "    write_list_to_file(dsts, f\"dest_{pid}.txt\")\n",
    "    #print(len(dsts))\n",
    "\n",
    "    # compute on mesh A\n",
    "    out_a = f\"tmp_a_{pid}.txt\"\n",
    "    subprocess.call([\n",
    "        \"DGG_LC.exe\",\n",
    "        f\"DGGData\\\\smal_{ma}_FD0.0100000000_c20.binary\",\n",
    "        f\"source_{pid}.txt\",f\"dest_{pid}.txt\", out_a, \"dij\",\"flt\"\n",
    "    ])\n",
    "    #print(f\"DGGData\\\\human_{ma}_FD0.0100000000_c20.binary\")\n",
    "    if(os.path.exists(out_a) == False):\n",
    "        continue\n",
    "    da = read_floats(out_a)\n",
    "    os.remove(out_a)\n",
    "    #print(len(da))\n",
    "    # compute on mesh B\n",
    "    out_b = f\"tmp_b_{pid}.txt\"\n",
    "    subprocess.call([\n",
    "        \"DGG_LC.exe\",\n",
    "        f\"DGGData\\\\smal_{mb}_FD0.0100000000_c20.binary\",\n",
    "        f\"source_{pid}.txt\",f\"dest_{pid}.txt\", out_b, \"dij\",\"flt\"\n",
    "    ])\n",
    "    if(os.path.exists(out_b) == False):\n",
    "        continue\n",
    "    #print(f\"DGGData\\\\human_{mb}_FD0.0100000000_c20.binary\")\n",
    "    db = read_floats(out_b)\n",
    "    os.remove(out_b)\n",
    "    #print(len(db))\n",
    "    #os.remove(out_b)\n",
    "    #os.remove(out_a)\n",
    "    # store block\n",
    "    block_size = NUM_SOURCES * NUM_DESTS_PER_SOURCE\n",
    "    sl = slice(idx, idx + block_size)\n",
    "    mesh_a[sl] = ma\n",
    "    mesh_b[sl] = mb\n",
    "    sources[sl] = np.repeat(srcs, NUM_DESTS_PER_SOURCE)\n",
    "    dests[sl]   = dsts\n",
    "    dist_a[sl]  = da\n",
    "    dist_b[sl]  = db\n",
    "\n",
    "    idx += block_size\n",
    "\n",
    "    # clean up tmp files\n",
    "    \n",
    "    \n",
    "\n",
    "# finally, save compressed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d95494c-22b7-41fd-8ef8-e818fd2defc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159976000\n"
     ]
    }
   ],
   "source": [
    "# For int16 arrays, check if elements are NOT the sentinel value (-1)\n",
    "is_sources_set = (sources != -1)\n",
    "is_dests_set = (dests != -1)\n",
    "is_mesha_set = (mesh_a!= -1)\n",
    "is_meshb_set = (mesh_b!= -1)\n",
    "# ... and so on for mesh_a, mesh_b\n",
    "\n",
    "# For float16 arrays, check if elements are NOT NaN.\n",
    "# np.isnan() returns True for NaN values, so we use ~ (NOT)\n",
    "is_dist_a_set = ~np.isnan(dist_a)\n",
    "is_dist_b_set = ~np.isnan(dist_b)\n",
    "\n",
    "# Combine masks based on your definition of a \"set\" element/row.\n",
    "# Example: A row is considered 'set' if its 'sources' and 'dist_a' values are both set.\n",
    "# Adjust this 'combined_mask' logic based on your actual data dependencies.\n",
    "combined_mask = is_sources_set & is_dist_a_set & is_mesha_set & is_meshb_set\n",
    "\n",
    "# Filter all arrays using the combined mask\n",
    "filtered_mesh_a = mesh_a[combined_mask]\n",
    "filtered_mesh_b = mesh_b[combined_mask]\n",
    "filtered_sources = sources[combined_mask]\n",
    "filtered_dests = dests[combined_mask]\n",
    "filtered_dist_a = dist_a[combined_mask]\n",
    "filtered_dist_b = dist_b[combined_mask]\n",
    "print(np.sum(combined_mask))\n",
    "np.savez_compressed(\n",
    "    \"geodesic_data_0.npz\",\n",
    "    mesh_a=filtered_mesh_a,\n",
    "    mesh_b=filtered_mesh_b,\n",
    "    source=filtered_sources,\n",
    "    dest=filtered_dests,\n",
    "    dist_on_a=filtered_dist_a,\n",
    "    dist_on_b=filtered_dist_b\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25019c66-40c7-4562-9c2d-d462c191fb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159976000\n"
     ]
    }
   ],
   "source": [
    "combined_mask = is_sources_set & is_dist_a_set & is_mesha_set & is_meshb_set & is_dist_b_set & is_dests_set\n",
    "print(np.sum(combined_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190cfccc-b275-4857-bd71-8510753d83aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
